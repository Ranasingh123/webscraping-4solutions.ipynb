{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de83c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import debugger\n",
    "import re\n",
    "\n",
    "# selenium\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "# Beautiful soup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# add time\n",
    "import time\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2231265e",
   "metadata": {},
   "source": [
    "# Q1 : Scrape the details of most viewed videos on YouTube from Wikipedia:\n",
    " Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/\n",
    " You need to find following details:\n",
    " A) Rank\n",
    " B) Name\n",
    " C) Artist\n",
    " D) Upload date\n",
    " E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08268a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12332\\2997218116.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\Dell\\Downloads\\chromedriver_win32 (1)\\chromedriver_win32.exe\")\n"
     ]
    }
   ],
   "source": [
    "# first, connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Dell\\Downloads\\chromedriver_win32 (1)\\chromedriver_win32.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00e7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list for scraping the data\n",
    "\n",
    "Rank = []\n",
    "Name = []\n",
    "Artist = []\n",
    "Date = []\n",
    "Views = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8699226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views (in Billions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Baby Shark Dance</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>12.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Despacito</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Johny Johny Yes Papa</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>Bath Song</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>6.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>5.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>See You Again</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>Phonics Song with Two Words</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>Wheels on the Bus</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>Uptown Funk</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>4.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>Learning Colors – Colorful Eggs on a Farm</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>Gangnam Style</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>Masha and the Bear – Recipe for Disaster</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>Dame Tu Cosita</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>Axel F</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>Roar</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>Counting Stars</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>Sorry</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>Thinking Out Loud</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>Baa Baa Black Sheep</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>Waka Waka (This Time for Africa)</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>Dark Horse</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>Faded</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>Let Her Go</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>Girls Like You</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>Bailando</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>Lean On</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>Humpty the train on a fruits ride</td>\n",
       "      <td>Kiddiestv Hindi – Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>Lakdi Ki Kathi</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>June 14, 2018</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                       Name  \\\n",
       "0    1.                           Baby Shark Dance   \n",
       "1    2.                                  Despacito   \n",
       "2    3.                       Johny Johny Yes Papa   \n",
       "3    4.                                  Bath Song   \n",
       "4    5.                               Shape of You   \n",
       "5    6.                              See You Again   \n",
       "6    7.                Phonics Song with Two Words   \n",
       "7    8.                          Wheels on the Bus   \n",
       "8    9.                                Uptown Funk   \n",
       "9   10.  Learning Colors – Colorful Eggs on a Farm   \n",
       "10  11.                              Gangnam Style   \n",
       "11  12.   Masha and the Bear – Recipe for Disaster   \n",
       "12  13.                             Dame Tu Cosita   \n",
       "13  14.                                      Sugar   \n",
       "14  15.                                     Axel F   \n",
       "15  16.                                       Roar   \n",
       "16  17.                             Counting Stars   \n",
       "17  18.                                      Sorry   \n",
       "18  19.                          Thinking Out Loud   \n",
       "19  20.                        Baa Baa Black Sheep   \n",
       "20  21.           Waka Waka (This Time for Africa)   \n",
       "21  22.                                 Dark Horse   \n",
       "22  23.                                      Faded   \n",
       "23  24.                                 Let Her Go   \n",
       "24  25.                             Girls Like You   \n",
       "25  26.                                    Perfect   \n",
       "26  27.                                   Bailando   \n",
       "27  28.                                    Lean On   \n",
       "28  29.          Humpty the train on a fruits ride   \n",
       "29  30.                             Lakdi Ki Kathi   \n",
       "\n",
       "                                           Artist        Upload Date  \\\n",
       "0     Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016   \n",
       "1                                      Luis Fonsi   January 12, 2017   \n",
       "2                                     LooLoo Kids    October 8, 2016   \n",
       "3                      Cocomelon – Nursery Rhymes        May 2, 2018   \n",
       "4                                      Ed Sheeran   January 30, 2017   \n",
       "5                                     Wiz Khalifa      April 6, 2015   \n",
       "6                                       ChuChu TV      March 6, 2014   \n",
       "7                      Cocomelon – Nursery Rhymes       May 24, 2018   \n",
       "8                                     Mark Ronson  November 19, 2014   \n",
       "9                                     Miroshka TV  February 27, 2018   \n",
       "10                                            Psy      July 15, 2012   \n",
       "11                                     Get Movies   January 31, 2012   \n",
       "12                                      El Chombo      April 5, 2018   \n",
       "13                                       Maroon 5   January 14, 2015   \n",
       "14                                     Crazy Frog      June 16, 2009   \n",
       "15                                     Katy Perry  September 5, 2013   \n",
       "16                                    OneRepublic       May 31, 2013   \n",
       "17                                  Justin Bieber   October 22, 2015   \n",
       "18                                     Ed Sheeran    October 7, 2014   \n",
       "19                     Cocomelon – Nursery Rhymes      June 25, 2018   \n",
       "20                                        Shakira       June 4, 2010   \n",
       "21                                     Katy Perry  February 20, 2014   \n",
       "22                                    Alan Walker   December 3, 2015   \n",
       "23                                      Passenger      July 25, 2012   \n",
       "24                                       Maroon 5       May 31, 2018   \n",
       "25                                     Ed Sheeran   November 9, 2017   \n",
       "26                               Enrique Iglesias     April 11, 2014   \n",
       "27                                    Major Lazer     March 22, 2015   \n",
       "28  Kiddiestv Hindi – Nursery Rhymes & Kids Songs   January 26, 2018   \n",
       "29                                   Jingle Toons      June 14, 2018   \n",
       "\n",
       "   Views (in Billions)  \n",
       "0                12.27  \n",
       "1                 8.08  \n",
       "2                 6.61  \n",
       "3                 6.01  \n",
       "4                 5.91  \n",
       "5                 5.78  \n",
       "6                 5.15  \n",
       "7                 4.92  \n",
       "8                 4.83  \n",
       "9                 4.81  \n",
       "10                4.69  \n",
       "11                4.53  \n",
       "12                4.23  \n",
       "13                3.82  \n",
       "14                3.75  \n",
       "15                3.73  \n",
       "16                3.72  \n",
       "17                3.63  \n",
       "18                3.55  \n",
       "19                3.49  \n",
       "20                3.47  \n",
       "21                3.45  \n",
       "22                3.40  \n",
       "23                3.38  \n",
       "24                3.37  \n",
       "25                3.37  \n",
       "26                3.33  \n",
       "27                3.33  \n",
       "28                3.30  \n",
       "29                3.30  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scraping Rank of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(\"xpath\",\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[1]\"):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"-\")\n",
    "        \n",
    "# Scraping Name of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(\"xpath\",\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[2]\"):\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append(\"-\")\n",
    "        \n",
    "# Scraping Artist of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(\"xpath\",\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[3]\"):\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Artist.append(\"-\")\n",
    "        \n",
    "# Scraping Upload_Date of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(\"xpath\",\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[5]\"):\n",
    "        Date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Date.append(\"-\")\n",
    "        \n",
    "# Scraping Views of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(\"xpath\",\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[4]\"):\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Views.append(\"-\")\n",
    "        \n",
    "# creating DataFrame for scraped data\n",
    "Wiki = pd.DataFrame({})\n",
    "Wiki['Rank'] = Rank\n",
    "Wiki['Name'] = Name\n",
    "Wiki['Artist'] = Artist\n",
    "Wiki['Upload Date'] = Date\n",
    "Wiki['Views (in Billions)'] = Views\n",
    "\n",
    "# removing stray numbers from Name column\n",
    "Wiki.Name = Wiki.Name.apply(lambda x:x[:-4].strip('\"'))\n",
    "Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d157fe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30 30 30\n"
     ]
    }
   ],
   "source": [
    "print(len(Rank),\n",
    "len(Name),\n",
    "len(Artist),\n",
    "len(Date),\n",
    "len(Views))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35089ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a58e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "febeec9b",
   "metadata": {},
   "source": [
    "# Q2 : Scrape the details team India’s international fixtures from bcci.tv.\n",
    " Url = https://www.bcci.tv/.\n",
    " You need to find following details:\n",
    "  A) Match title (I.e. 1st ODI)\n",
    "  B) Series\n",
    "  C) Place\n",
    "  D) Date\n",
    "  E) Time\n",
    "\n",
    "    Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d155419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Dell\\Downloads\\chromedriver_win32 (1)\\chromedriver_win32 (1)\")\n",
    "\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url=('https://www.bcci.tv/')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn=driver.find_elements(\"xpath\",\"//div[@class='navigation__drop-down drop-down drop-down--reveal-on-hover']/div/ul/li/a\")\n",
    "driver.getAttribute(\"href\")\n",
    "time.sleep(3)\n",
    "\n",
    "# creating empty lists for scraping the data\n",
    "Match_Title = []\n",
    "Series = []\n",
    "Place = []\n",
    "Date = []\n",
    "Time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fbec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in driver.find_elements_by_xpath(\"//div[@class='fixture__format-strip']/span[@class='u-unskewed-text fixture__format']\"):\n",
    "    Match_Title.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='fixture__format-strip']/span[@class='u-unskewed-text fixture__tournament-label u-truncated']\"):\n",
    "    Series.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='fixture__description u-unskewed-text']/p/span\"):\n",
    "    Place.append(i.text)\n",
    "        \n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='fixture__datetime tablet-only']/strong[1]\"):\n",
    "    Date.append(i.text.replace('\\n',' '))\n",
    "\n",
    "date=[i.split(' ',3)[:3] for i in Date]\n",
    "date=[' '.join(i) for i in date]\n",
    "Time=[i.split(' ',3)[-1] for i in Date]\n",
    "\n",
    "# creating data frame\n",
    "fixture=pd.DataFrame({'Match Title': Match_Title,\n",
    "                          \"Series\": Series,\n",
    "                          \"Place\": Place,\n",
    "                          \"Date\": date,\n",
    "                          \"Time\": Time})\n",
    "fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8683bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ecb7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4feebfa",
   "metadata": {},
   "source": [
    "# Q 3: Scrape the details of selenium exception from guru99.com.\n",
    " Url = https://www.guru99.com/\n",
    " You need to find following details:\n",
    " A) Name\n",
    " B) Description\n",
    " Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19749962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.guru99.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "Name = []\n",
    "Description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06bbd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on Selenium button\n",
    "driver.find_element_by_xpath(\"//li//a[@title='Selenium']\").click()\n",
    "\n",
    "# clicking on Exception Handling button\n",
    "driver.find_element_by_xpath('//a[@title=\"Selenium Exception Handling (Common Exceptions List)\"]').click()\n",
    "\n",
    "# scraping Name\n",
    "for i in driver.find_elements_by_xpath(\"//table[@class='table table-striped']/tbody/tr/td[1]\"):\n",
    "    Name.append(i.text)\n",
    "    \n",
    "# scraping Description\n",
    "for i in driver.find_elements_by_xpath(\"//table[@class='table table-striped']/tbody/tr/td[2]\"):\n",
    "    Description.append(i.text)\n",
    "\n",
    "    \n",
    "# creating the dataframe from the scraped data\n",
    "Selenium = pd.DataFrame({})\n",
    "Selenium['Exception_Name'] = Name\n",
    "Selenium['Description'] = Description\n",
    "Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995bbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7e0f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecc618e0",
   "metadata": {},
   "source": [
    "# Q 4 : Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "  Url = http://statisticstimes.com/\n",
    "  You have to find following details:\n",
    "  A) Rank\n",
    "  B) State\n",
    "  C) GSDP at current price (19-20)\n",
    "  D) GSDP at current price (18-19)\n",
    "  E) Share(18-19)\n",
    "  F) GDP($ billion)\n",
    "    Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://statisticstimes.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on Economy button\n",
    "driver.find_element_by_xpath(\"//div[@class='navbar']/div[2]/button\").click()\n",
    "\n",
    "# clicking on India\n",
    "driver.find_element_by_xpath(\"//div[@class='dropdown-content']/a[3]\").click()\n",
    "time.sleep(3)\n",
    "\n",
    "# clicking on GDP of Indian Economy\n",
    "GDP = driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e12564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "Rank = []\n",
    "State = []\n",
    "GSDP1 = []\n",
    "GSDP2 = []\n",
    "Share = []\n",
    "GDP_billion = []\n",
    "\n",
    "# scraping Rank\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[1]\"):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"_\")\n",
    "    \n",
    "# scraping State\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[2]\"):\n",
    "        State.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    State.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (19-20)\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[3]\"):\n",
    "        GSDP1.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP1.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[4]\"):\n",
    "        GSDP2.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP2.append(\"_\")\n",
    "    \n",
    "# scraping Share (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[5]\"):\n",
    "        Share.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Share.append(\"_\")\n",
    "    \n",
    "# scraping GDP $ billion\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[6]\"):\n",
    "        GDP_billion.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDP_billion.append(\"_\")\n",
    "    \n",
    "    \n",
    "# creating DataFrame from the scraped data\n",
    "GDP = pd.DataFrame({})\n",
    "GDP['Rank'] = Rank\n",
    "GDP['State'] = State\n",
    "GDP['GSDP at current price (19-20)'] = GSDP1\n",
    "GDP['GSDP at current price (18-19)'] = GSDP2\n",
    "GDP['Share (18-19)'] = Share\n",
    "GDP['GDP($ billion)'] = GDP_billion\n",
    "GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9814671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f205ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2833da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "769b2570",
   "metadata": {},
   "source": [
    "#Q 5 : Scrape the details of trending repositories on Github.com.\n",
    "  Url = https://github.com/\n",
    "  You have to find the following details:\n",
    "  A) Repository title\n",
    "  B) Repository description\n",
    "  C) Contributors count\n",
    "  D) Language used\n",
    "    Note: - From the home page you have to click on the trending option from Explore menu through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07acb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://github.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6242675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "URLs = []\n",
    "repository_title = []\n",
    "Description = []\n",
    "Contributors = []\n",
    "Language = []\n",
    "lang = []\n",
    "\n",
    "# fetching urls for each repository\n",
    "repository = driver.find_elements_by_xpath(\"//h1[@class='h3 lh-condensed']//a\")\n",
    "for i in repository:\n",
    "    URLs.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "# scraping Repository title data\n",
    "title = driver.find_elements_by_xpath(\"//h1[@class = 'h3 lh-condensed']\")\n",
    "for i in title:\n",
    "    repository_title.append(i.text)\n",
    "    \n",
    "# scraping data from all repository page\n",
    "for i in URLs:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # scraping Repository Description data \n",
    "    try:\n",
    "        desc = driver.find_element_by_xpath(\"//p[@class='f4 mt-3']\")\n",
    "        Description.append(desc.text)\n",
    "    except NoSuchElementException:\n",
    "        Description.append('-')\n",
    "        \n",
    "        \n",
    "    # scraping Contributors Count data\n",
    "    try:\n",
    "        contributor = driver.find_element_by_xpath(\"//*[contains(text(),'    Contributors ')]\")\n",
    "        Contributors.append(contributor.text.replace('Contributors',''))\n",
    "    except NoSuchElementException:\n",
    "        Contributors.append('-')\n",
    "    \n",
    "    \n",
    "    # scraping Languages used data\n",
    "    try:\n",
    "        for i in driver.find_elements_by_xpath(\"//ul[@class= 'list-style-none']//li//span[1]\"):\n",
    "            lang.append(i.text)\n",
    "        Language.append(lang)\n",
    "    except NoSuchElementException:\n",
    "        Language.append('-')\n",
    "        \n",
    "        \n",
    "# Data Framing\n",
    "Github = pd.DataFrame({})\n",
    "Github['Repository Title'] = repository_title\n",
    "Github['Repository Description'] = Description\n",
    "Github['Contributors Count'] = Contributors\n",
    "Github['Language Used'] = Language\n",
    "Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf23049",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222691ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c86de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2571d4d",
   "metadata": {},
   "source": [
    "# Q 6 : 6. Scrape the details of top 100 songs on billboard.com.\n",
    "     Url = https://www.billboard.com/\n",
    "     You have to find the following details:\n",
    "     A) Song name\n",
    "     B) Artist name\n",
    "     C) Last week rank\n",
    "     D) Peak rank\n",
    "     E) Weeks on board\n",
    "      Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ccc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.billboard.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ab7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on option button\n",
    "charts=driver.find_element_by_xpath(\"//a[@class='header__main-link header__main-link--charts']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04387b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Song_Name = []\n",
    "Artist_Name =[]\n",
    "Last_week_rank = []\n",
    "Peak_rank = []\n",
    "Weeks_on_board = []\n",
    "\n",
    "# getting urls for top 100 songs\n",
    "urls = driver.find_element_by_xpath(\"//li[@class='header__submenu__list__element']//a\")\n",
    "page_url = urls.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(4)\n",
    "\n",
    "# scraping data of song names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='chart-element__information__song text--truncate color--primary']\"):\n",
    "    Song_Name.append(i.text)\n",
    "    \n",
    "# scraping data of artist names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='chart-element__information__artist text--truncate color--secondary']\"):\n",
    "    Artist_Name.append(i.text)\n",
    "    \n",
    "# scraping data of last week ranks\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--last']\"):\n",
    "    Last_week_rank.append(i.text)\n",
    "    \n",
    "\n",
    "# scraping data of peak ranks\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--peak']\"):\n",
    "    Peak_rank.append(i.text)       \n",
    "    \n",
    "    \n",
    "# scraping data of weeks on board\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--week']\"):\n",
    "    Weeks_on_board.append(i.text)\n",
    "    \n",
    "    \n",
    "# creating dataframe for scraped data\n",
    "billiboard = pd.DataFrame({})\n",
    "billiboard['Name'] = Song_Name\n",
    "billiboard['Artist'] = Artist_Name\n",
    "billiboard['Last Week Rank'] = Last_week_rank\n",
    "billiboard['Peak Rank'] = Peak_rank\n",
    "billiboard['Weeks on board'] = Weeks_on_board\n",
    "billiboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf752d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307ad6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a66d99",
   "metadata": {},
   "source": [
    "# Q 7 : Scrape the details of Data science recruiters from naukri.com.\n",
    "  Url = https://www.naukri.com/\n",
    "  You have to find the following details:\n",
    "  A) Name\n",
    "  B) Designation\n",
    "  C) Company\n",
    "  D) Skills they hire for\n",
    "  E) Location\n",
    "    Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "038013c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12332\\2667530360.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.naukri.com/\")\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7391c31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getAttribute' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# fetching urls to navigate recruiter page\u001b[39;00m\n\u001b[0;32m      2\u001b[0m recruiter \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpath\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//a[@title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSearch Recruiters\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m page_url \u001b[38;5;241m=\u001b[39m \u001b[43mgetAttribute\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(page_url)\n\u001b[0;32m      6\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'getAttribute' is not defined"
     ]
    }
   ],
   "source": [
    "# fetching urls to navigate recruiter page\n",
    "recruiter = driver.find_elements(\"xpath\",\"//a[@title='Search Recruiters']\")\n",
    "page_url = getAttribute(\"href\")\n",
    "\n",
    "driver.get(page_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2ba6f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Company</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, Designation, Company, Skills, Location]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty lists\n",
    "Name = []\n",
    "Designation = []\n",
    "Company = []\n",
    "Skills = []\n",
    "Location = []\n",
    "\n",
    "# scraping data of Names\n",
    "for i in driver.find_elements(\"xpath\",\"//span[@class='fl ellipsis']\"):\n",
    "    Name.append(i.text)\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Designation\n",
    "for i in driver.find_elements(\"xpath\",\"//span[@class='ellipsis clr']\"):\n",
    "    Designation.append(i.text)\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Company Name\n",
    "for i in driver.find_elements(\"xpath\",\"//div[@class='vcard']//p[1]/a[2]\"):\n",
    "    Company.append(i.text)\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Skills\n",
    "for i in driver.find_elements(\"xpath\",\"//div[@class='hireSec highlightable']\"):\n",
    "    try:\n",
    "        if i.text == \"Not Specified\": raise NoSuchElementException\n",
    "        Skills.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Skills.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Location\n",
    "for i in driver.find_elements(\"xpath\",\"//div[@class='vcard']//p[1]/span/small\"):\n",
    "    try:\n",
    "        if i.text == \"Not Specified\": raise NoSuchElementException\n",
    "        Location.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Location.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# creating dataframe for scraped data\n",
    "Naukri = pd.DataFrame({})\n",
    "Naukri['Name'] = Name[:49]\n",
    "Naukri['Designation'] = Designation[:49]\n",
    "Naukri['Company'] = Company[:49]\n",
    "Naukri['Skills'] = Skills[:49]\n",
    "Naukri['Location'] = Location[:49]\n",
    "Naukri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dd8df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c665a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123a3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "460195b6",
   "metadata": {},
   "source": [
    "# Q 8 : Scrape the details of Highest selling novels.\n",
    "  Url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\"\n",
    "  You have to find the following details:\n",
    "  A) Book name\n",
    "  B) Author name\n",
    "  C) Volumes sold\n",
    "  D) Publisher\n",
    "  E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab8c5f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12332\\4260876969.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"‪C:\\Users\\Dell\\Downloads\\chromedriver_win32 (1)\\chromedriver_win32 (1)\")\n"
     ]
    }
   ],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"‪C:\\Users\\Dell\\Downloads\\chromedriver_win32 (1)\\chromedriver_win32 (1)\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\")\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64310e1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_elements_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m Genre \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# scraping book names data\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements_by_xpath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//tbody//tr//td[2]\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     11\u001b[0m     Book_name\u001b[38;5;241m.\u001b[39mappend(i\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# scraping author names data\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_elements_by_xpath'"
     ]
    }
   ],
   "source": [
    "# creating empty lists\n",
    "Book_name = []\n",
    "Author_name = []\n",
    "Volumes_sold = []\n",
    "Publisher = []\n",
    "Genre = []\n",
    "\n",
    "\n",
    "# scraping book names data\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr//td[2]\"):\n",
    "    Book_name.append(i.text)\n",
    "\n",
    "    \n",
    "# scraping author names data\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr//td[3]\"):\n",
    "    try:\n",
    "        if i.text == '0' : raise NoSuchElementException\n",
    "        Author_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Author_name.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of volumes sold\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr//td[4]\"):\n",
    "    Volumes_sold.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraping data of publisher names\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr//td[5]\"):\n",
    "    Publisher.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraping  data of genre\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr//td[6]\"):\n",
    "    Genre.append(i.text)    \n",
    "    \n",
    "    \n",
    "# creating dataframe for scraped data\n",
    "Novels = pd.DataFrame({})\n",
    "Novels['Book Name'] = Book_name\n",
    "Novels['Author'] = Author_name\n",
    "Novels['Volume sold'] = Volumes_sold\n",
    "Novels['Publisher'] = Publisher\n",
    "Novels['Genre'] = Genre\n",
    "Novels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2611dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b16d5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74082b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a77fe8",
   "metadata": {},
   "source": [
    "# Q 9 : Scrape the details most watched tv series of all time from imdb.com.\n",
    "  Url = https://www.imdb.com/list/ls095964455/\n",
    "  You have to find the following details:\n",
    "  A) Name\n",
    "  B) Year span\n",
    "  C) Genre\n",
    "  D) Run time\n",
    "  E) Ratings\n",
    "  F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49dafb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12332\\380918905.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.imdb.com/list/ls095964455/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3365fb9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_elements_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m Votes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# scraped data of Names\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements_by_xpath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//h3[@class=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlister-item-header\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]/a\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     11\u001b[0m     Name\u001b[38;5;241m.\u001b[39mappend(i\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# scraped data of Year span\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_elements_by_xpath'"
     ]
    }
   ],
   "source": [
    "# creating empty lists\n",
    "Name = []\n",
    "Year_span = []\n",
    "Genre = []\n",
    "Run_time = []\n",
    "Ratings = []\n",
    "Votes = []\n",
    "\n",
    "# scraped data of Names\n",
    "for i in driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/a\"):\n",
    "    Name.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraped data of Year span\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='lister-item-year text-muted unbold']\"):\n",
    "    Year_span.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraped data of Genre\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='genre']\"):\n",
    "    Genre.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraped data of Run time\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='runtime']\"):\n",
    "    Run_time.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraped data of Ratings\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='ipl-rating-star small']//span[2]\"):\n",
    "    Ratings.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraped data of Votes\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='lister-item-content']//p[4]/span[2]\"):\n",
    "    Votes.append(i.text) \n",
    "    \n",
    "    \n",
    "# creating dataframe for scraped data\n",
    "TV_Series = pd.DataFrame({})\n",
    "TV_Series['Name'] = Name\n",
    "TV_Series['Year Span'] = Year_span\n",
    "TV_Series['Genre'] = Genre\n",
    "TV_Series['Run Time'] = Run_time\n",
    "TV_Series['Ratings'] = Ratings\n",
    "TV_Series['Votes'] = Votes\n",
    "TV_Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf55f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121fedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe1a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7e13fc6",
   "metadata": {},
   "source": [
    "# Q 10 : Details of Datasets from UCI machine learning repositories.\n",
    "   Url = https://archive.ics.uci.edu/\n",
    "   You have to find the following details:\n",
    "   A) Dataset name\n",
    "   B) Data type\n",
    "   C) Task\n",
    "   D) Attribute type\n",
    "   E) No of instances\n",
    "   F) No of attribute\n",
    "   G) Year\n",
    "    Note: - from the home page you have to go to the Show All Dataset page through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e785a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12332\\942346772.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)\\chromedriver_win32 (1).exe\")\n"
     ]
    }
   ],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)\\chromedriver_win32 (1).exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\" https://archive.ics.uci.edu/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b643e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching view all dataset button from the webpage\n",
    "viewall_dataset = driver.find_element(\"xpath\",\"//tbody[1]//tr/td[2]/span[2]/a\")\n",
    "page_url = viewall_dataset.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b64089d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching page urls of all datasets\n",
    "view_list = driver.find_element(\"xpath\",\"/html/body/table[2]/tbody/tr/td[2]/table[1]/tbody/tr/td[2]/p/a\")\n",
    "list_url = view_list.get_attribute(\"href\")\n",
    "driver.get(list_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae61e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching urls for each dataset\n",
    "dataset_url = driver.find_elements(\"xpath\",\"//p[@class='normal']//b/a\")\n",
    "\n",
    "urls = []\n",
    "for i in dataset_url:\n",
    "    urls.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cc0e87d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: timeout: Timed out receiving message from renderer: 600.000\n  (Session info: chrome=109.0.5414.120)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x00A16643]\n\t(No symbol) [0x009ABE21]\n\t(No symbol) [0x008ADA9D]\n\t(No symbol) [0x0089F55A]\n\t(No symbol) [0x0089F2D8]\n\t(No symbol) [0x0089DC68]\n\t(No symbol) [0x0089E512]\n\t(No symbol) [0x0089D205]\n\t(No symbol) [0x008A91BC]\n\t(No symbol) [0x0089D0D5]\n\t(No symbol) [0x0089EE5A]\n\t(No symbol) [0x0089DC68]\n\t(No symbol) [0x0089E512]\n\t(No symbol) [0x0089D205]\n\t(No symbol) [0x008A671C]\n\t(No symbol) [0x0089D0D5]\n\t(No symbol) [0x0089EE5A]\n\t(No symbol) [0x0089DC68]\n\t(No symbol) [0x0089E512]\n\t(No symbol) [0x0089D205]\n\t(No symbol) [0x008A40E2]\n\t(No symbol) [0x0089D0D5]\n\t(No symbol) [0x0089EE5A]\n\t(No symbol) [0x0089DC68]\n\t(No symbol) [0x0089E512]\n\t(No symbol) [0x0089D205]\n\t(No symbol) [0x00895606]\n\t(No symbol) [0x0089D0D5]\n\t(No symbol) [0x0089C670]\n\t(No symbol) [0x0089C7BF]\n\t(No symbol) [0x0089CC94]\n\t(No symbol) [0x0089CAD8]\n\t(No symbol) [0x008AF253]\n\t(No symbol) [0x00916200]\n\t(No symbol) [0x008FFB76]\n\t(No symbol) [0x008D49C1]\n\t(No symbol) [0x008D5E5D]\n\tGetHandleVerifier [0x00C8A142+2497106]\n\tGetHandleVerifier [0x00CB85D3+2686691]\n\tGetHandleVerifier [0x00CBBB9C+2700460]\n\tGetHandleVerifier [0x00AC3B10+635936]\n\t(No symbol) [0x009B4A1F]\n\t(No symbol) [0x009BA418]\n\t(No symbol) [0x009BA505]\n\t(No symbol) [0x009C508B]\n\tBaseThreadInitThunk [0x754F00F9+25]\n\tRtlGetAppContainerNamedObjectPath [0x775B7BBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x775B7B8E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# scraping Year\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m     year \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//table[@border=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]//tbody/tr[2]/td[6]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m year\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException\n\u001b[0;32m     82\u001b[0m     Year\u001b[38;5;241m.\u001b[39mappend(year\u001b[38;5;241m.\u001b[39mtext[:\u001b[38;5;241m4\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:830\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    827\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    828\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    438\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    243\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: timeout: Timed out receiving message from renderer: 600.000\n  (Session info: chrome=109.0.5414.120)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x00A16643]\n\t(No symbol) [0x009ABE21]\n\t(No symbol) [0x008ADA9D]\n\t(No symbol) [0x0089F55A]\n\t(No symbol) [0x0089F2D8]\n\t(No symbol) [0x0089DC68]\n\t(No symbol) [0x0089E512]\n\t(No symbol) [0x0089D205]\n\t(No symbol) [0x008A91BC]\n\t(No symbol) [0x0089D0D5]\n\t(No symbol) [0x0089EE5A]\n\t(No symbol) [0x0089DC68]\n\t(No symbol) [0x0089E512]\n\t(No symbol) [0x0089D205]\n\t(No symbol) [0x008A671C]\n\t(No symbol) [0x0089D0D5]\n\t(No symbol) [0x0089EE5A]\n\t(No symbol) [0x0089DC68]\n\t(No symbol) [0x0089E512]\n\t(No symbol) [0x0089D205]\n\t(No symbol) [0x008A40E2]\n\t(No symbol) [0x0089D0D5]\n\t(No symbol) [0x0089EE5A]\n\t(No symbol) [0x0089DC68]\n\t(No symbol) [0x0089E512]\n\t(No symbol) [0x0089D205]\n\t(No symbol) [0x00895606]\n\t(No symbol) [0x0089D0D5]\n\t(No symbol) [0x0089C670]\n\t(No symbol) [0x0089C7BF]\n\t(No symbol) [0x0089CC94]\n\t(No symbol) [0x0089CAD8]\n\t(No symbol) [0x008AF253]\n\t(No symbol) [0x00916200]\n\t(No symbol) [0x008FFB76]\n\t(No symbol) [0x008D49C1]\n\t(No symbol) [0x008D5E5D]\n\tGetHandleVerifier [0x00C8A142+2497106]\n\tGetHandleVerifier [0x00CB85D3+2686691]\n\tGetHandleVerifier [0x00CBBB9C+2700460]\n\tGetHandleVerifier [0x00AC3B10+635936]\n\t(No symbol) [0x009B4A1F]\n\t(No symbol) [0x009BA418]\n\t(No symbol) [0x009BA505]\n\t(No symbol) [0x009C508B]\n\tBaseThreadInitThunk [0x754F00F9+25]\n\tRtlGetAppContainerNamedObjectPath [0x775B7BBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x775B7B8E+238]\n"
     ]
    }
   ],
   "source": [
    "# creating empty lists\n",
    "Dataset_name = []\n",
    "Data_type = []\n",
    "Task = []\n",
    "Attribute_type = []\n",
    "No_of_instances = []\n",
    "No_of_attributes = []\n",
    "Year = []\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping  Dataset name\n",
    "    try:\n",
    "        dataset_name = driver.find_element(\"xpath\",\"//span[@class='heading']\")\n",
    "        Dataset_name.append(dataset_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Dataset_name.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping data type\n",
    "    try:\n",
    "        data_type = driver.find_element(\"xpath\",\"//table[@border='1']//tbody/tr/td[2]\")\n",
    "        if data_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Data_type.append(data_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping Task\n",
    "    try:\n",
    "        task = driver.find_element(\"xpath\",\"//table[@border='1']//tbody/tr[3]/td[2]\")\n",
    "        if task.text == \"N/A\": raise NoSuchElementException\n",
    "        Task.append(task.text)\n",
    "    except NoSuchElementException:\n",
    "        Task.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping Attribute type\n",
    "    try:\n",
    "        attribute_type = driver.find_element(\"xpath\",\"//table[@border='1']//tbody/tr[2]/td[2]\")\n",
    "        if attribute_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Attribute_type.append(attribute_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Attribute_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping No of Instances\n",
    "    try:\n",
    "        instances = driver.find_element(\"xpath\",\"//table[@border='1']//tbody/tr/td[4]\")\n",
    "        if instances.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_instances.append(instances.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_instances.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping No of Arrtibutes\n",
    "    try:\n",
    "        attribute = driver.find_element(\"xpath\",\"//table[@border='1']//tbody/tr[2]/td[4]\")\n",
    "        if attribute.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_attributes.append(attribute.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_attributes.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping Year\n",
    "    try:\n",
    "        year = driver.find_element(\"xpath\",\"//table[@border='1']//tbody/tr[2]/td[6]\")\n",
    "        if year.text == \"N/A\": raise NoSuchElementException\n",
    "        Year.append(year.text[:4])\n",
    "    except NoSuchElementException:\n",
    "        Year.append('-')\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe for scraped data\n",
    "ML = pd.DataFrame({})\n",
    "ML['Data Name'] = Data_name \n",
    "ML['Data Type '] = Data_type\n",
    "ML['Task '] = Task \n",
    "ML['Attribute Type '] = Attribute_type \n",
    "ML['No of Instance '] = No_of_instances\n",
    "ML['No of Attributes '] = No_of_attributes \n",
    "ML['Year '] = Year \n",
    "ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
